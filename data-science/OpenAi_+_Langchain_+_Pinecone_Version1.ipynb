# -*- coding: utf-8 -*-
"""OpenAi + Langchain + Pinecone Version1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18lEYNFr85L_yk8GsfgCDRe_C8vQ6tvTy

Author : Aryan Karande

# Reqirements.txt


1.   config
2.   langchain --upgrade
3.   python-dotenv
4.   openai
5.   tiktoken
6.   pyPDF
7.   pinecone-clent
"""

!pip install config

!pip install pinecone-client

!pip install langchain --upgrade
# Version: 0.0.164

!pip install pypdf

!pip install python-dotenv

!pip install openai

!pip install tiktoken

# PDF Loaders. If unstructured gives you a hard time, try PyPDFLoader
from langchain.document_loaders import UnstructuredPDFLoader, OnlinePDFLoader, PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from dotenv import load_dotenv
import os

load_dotenv()

!wget -q https://github.com/Hey-Aryan/Llama2-Pdf-Chatbot-/archive/refs/heads/main.zip
!unzip main.zip

"""### Load your data

Next let's load up some data. I've put a few 'loaders' on there which will load data from different locations. Feel free to use the one that suits you. The default one queries one of Paul Graham's essays for a simple example. This process will only stage the loader, not actually load it.
"""

#loader = TextLoader(file_path="../data/PaulGrahamEssays/vb.txt")

## Other options for loaders
loader = PyPDFLoader("/content/Llama2-Pdf-Chatbot--main/pdfs/GP9_2024.pdf")
# loader = UnstructuredPDFLoader("../data/field-guide-to-data-science.pdf")
# loader = OnlinePDFLoader("https://wolfpaulus.com/wp-content/uploads/2017/05/field-guide-to-data-science.pdf")

"""Then let's go ahead and actually load the data."""

data = loader.load()

"""Then let's actually check out what's been loaded


"""

# Note: If you're using PyPDFLoader then it will split by page for you already
print (f'You have {len(data)} document(s) in your data')
print (f'There are {len(data[0].page_content)} characters in your sample document')
print (f'Here is a sample: {data[0].page_content[:200]}')

"""# Chunk your data up into smaller documents

"""

# We'll split our data into chunks around 500 characters each with a 50 character overlap. These are relatively small.

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(data)

text = texts[0]

"""⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰
# Metadata
⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰⏰
"""

print(str(text.metadata["page"]))

print(text.page_content[:1000])

print(text.metadata["source"])

metadata = {'char_length': len(texts[112].page_content)}

metadata

text = texts[0]

metadata = {
    'char_length': len(text.page_content),
    'page_no': str(text.metadata["page"]),
    'text': text.page_content[:1000],
    'source': text.metadata["source"]
}

metadata

# Let's see how many small chunks we have
print (f'Now you have {len(texts)} documents')

"""### Create embeddings of your documents are documents later on."""

from langchain.vectorstores import Chroma, Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone

"""Check to see if there is an environment variable with you API keys, if not, use what you put below"""

OPENAI_API_KEY = os.getenv('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', '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')

"""Then we'll get our embeddings engine going. You can use whatever embeddings engine you would like. We'll use OpenAI's ada today."""



embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

""" ❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌

 # Delete all vectors from pinecone


❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌





"""

# pip install pinecone-client
from pinecone import Pinecone

pc = Pinecone(api_key="54fd135d-6e81-43c5-8802-ddfb63c09947")
index = pc.Index("avivo-vector-db")

index.delete(delete_all=True)

"""

---


# Version1 Function to upsert (ids, embeddings)

---

"""

import random
import itertools
from pinecone import Pinecone

pc = Pinecone(api_key="54fd135d-6e81-43c5-8802-ddfb63c09947")
index = pc.Index("avivo-vector-db")

def chunks(iterable, batch_size=100):
    """A helper function to break an iterable into chunks of size batch_size."""
    it = iter(iterable)
    embeddings = tuple(itertools.islice(it, batch_size))
    while embeddings:
        yield embeddings
        embeddings = tuple(itertools.islice(it, batch_size))

vector_dim = 1536
vector_count = 113

# Example generator that generates many (id, vector) pairs
example_data_generator = map(lambda i: (f'id-{i}', [random.random() for _ in range(vector_dim)]), range(vector_count))


# Upsert data with 100 vectors per upsert request
for ids_vectors_chunk in chunks(example_data_generator, batch_size=100):
    index.upsert(vectors=ids_vectors_chunk)

"""

---


# Version 2 To upsert (ids, embeddings, metadata)


---

"""

import random
import itertools
from pinecone import Pinecone

def get_pinecone_index():
    pc = Pinecone(api_key="54fd135d-6e81-43c5-8802-ddfb63c09947")
    index = pc.Index("avivo-vector-db")
    return index

from transformers import BertTokenizer, BertModel
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
stop_words = set(stopwords.words('english'))

"""⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪
# We don't need tokenization method !!
⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪⚪
"""

tokens = []
_tokens = tokenizer.tokenize('THE BOEING COMPANY GENERAL PROVISIONS  \nGP9 (Labor Hour/Time & Material Contract Under U.S. Government Prime Contract)  \n(Rev January 2024)  \n \nPage 1 of 15  \n1. FORMATION OF CONTRACT  \na. This proposed purchase contract, which incorporates by reference these General Provisions and all other terms and \nconditions set forth in this proposed purchase contract (collectively, “Contract ”), is Buyer ’s offer to purchase services  or \nmaterials  (collec tively, “Services ”) described in this offer . Acceptance is strictly limited to the terms and conditions in this \noffer. Unless specifically agreed to in writing by Buyer ’s Authorized Procurement Representative, Buyer objects to, and is \nnot bound by, any ter m or condition that differs from or adds to this offer . Seller’s commencement of performance or \nacceptance of this Contract in any manner shall conclusively evidence acceptance of this Contract as written. Seller’s')
for w in _tokens:
    if w not in stop_words:
        tokens.append(w)
tokens

"""✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅
# **how to get open ai embeddings in list or float**
✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅✅
"""

embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-ada-002")
text = texts[1].page_content
response = embeddings.embed_query(text)


response[:1536]

import time
def createVector(item):

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-ada-002")
    text = texts[item].page_content[:1000]
    response = embeddings.embed_query(text)
    text = texts[item]


    metadata = {
            'char_length': len(text.page_content),
            'page_no': str(text.metadata["page"]),
            'text': text.page_content[:1000],
            'source': text.metadata["source"]
        }

    vector = []
    vector.append({"id": str(item), "values": response[:1536], "metadata": metadata})
    index.upsert(vector)


for item in range (77,len(texts)):
    print(item)
    createVector(item)
    time.sleep(35)

### texts[0] -- texts[112]

"""

---
✅✅✅✅✅✅✅✅✅✅✅





# `Retrieval`



✅✅✅✅✅✅✅✅✅✅✅


---

"""

from openai import OpenAI

# get api key from platform.openai.com
openai.api_key = os.getenv('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') or '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'

embed_model = "text-embedding-ada-002"

"""# Retrieval Procedural Version 1"""

from pinecone import Pinecone

pc = Pinecone(api_key="54fd135d-6e81-43c5-8802-ddfb63c09947")
index = pc.Index("avivo-vector-db")
query = ("What is termination clause ? ")

embed = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-ada-002")
res = embeddings.embed_query(query)

# retrieve from Pinecone
xq = res[:1536]

# get relevant contexts (including the questions)
res = index.query(vector=xq, top_k=3, include_metadata=True)

res

"""# Retrieval Functional Aproach Version 2"""

query = ("What is termination clause ? explain in detail ")

limit = 3750
import time
from openai import OpenAI

OPENAI_API_KEY='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'
client = OpenAI(api_key=OPENAI_API_KEY)

def retrieve(query):
    embed = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-ada-002")
    res = embeddings.embed_query(query)

    # retrieve from Pinecone
    xq = res[:1536]

    # get relevant contexts
    contexts = []
    time_waited = 0
    while (len(contexts) < 3 and time_waited < 60 * 12):
        res = index.query(vector=xq, top_k=3, include_metadata=True)
        contexts = contexts + [
            x['metadata']['text'] for x in res['matches']
        ]
        print(f"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...")
        time.sleep(15)
        time_waited += 15

    if time_waited >= 60 * 12:
        print("Timed out waiting for contexts to be retrieved.")
        contexts = ["No contexts retrieved. Try to answer the question yourself!"]


    # build our prompt with the retrieved contexts included
    prompt_start = (
        "Answer the question based on the context below.\n\n"+
        "Context:\n"
    )
    prompt_end = (
        f"\n\nQuestion: {query}\nAnswer:"
    )
    # append contexts until hitting limit
    for i in range(1, len(contexts)):
        if len("\n\n---\n\n".join(contexts[:i])) >= limit:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts[:i-1]) +
                prompt_end
            )
            break
        elif i == len(contexts)-1:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts) +
                prompt_end
            )
    return prompt

def complete(prompt):
    # instructions
    sys_prompt = "You are a helpful assistant that always answers questions."
    # query text-davinci-003
    res = client.chat.completions.create(
        model='gpt-3.5-turbo-0125',
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt}
        ]
    )
    return res.choices[0].message.content

# first we retrieve relevant items from Pinecone
query_with_contexts = retrieve(query)
query_with_contexts

complete(query_with_contexts)

query2 = ("Explain me about PACKING AND SHIPPING")

query_with_contexts = retrieve(query2)
query_with_contexts

complete(query_with_contexts)

query3 = ("Explain me SCHEDULE")
query_with_contexts = retrieve(query3)
complete(query_with_contexts)

query4 = ("Till when is the WARRANTY valid ?")
query_with_contexts = retrieve(query4)
complete(query_with_contexts)

query5 = ("Till when is this contract valid ?")
query_with_contexts = retrieve(query5)
complete(query_with_contexts)

"""---



---



---



---



---



---

---



---



---



---



---



---



---

❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗❗

Limitations

1.   Can't store embeddings at once. Embeddings are stored chunck wise
2.   Doesn't have conversational memory
3.   No openai.chains used in converstion
4.   Can't input multiple pdfs

---



---



---



---



---



---

---



---



---



---



---



---



---
"""