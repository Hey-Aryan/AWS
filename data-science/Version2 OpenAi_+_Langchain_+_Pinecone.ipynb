# -*- coding: utf-8 -*-
"""Version2 OpenAi + Langchain + Pinecone  .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1npbUh-Kq7LBfoLqT_8FiykRuFuJ0TqQE

# Try to import Multiple pdfs, Multiple formats
"""

!pip install langchain

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --quiet  "unstructured[all-docs]"

# # Install other dependencies
# # https://github.com/Unstructured-IO/unstructured/blob/main/docs/source/installing.rst
!brew install libmagic
!brew install poppler
!brew install tesseract
# # If parsing xml / html documents:
!brew install libxml2
!brew install libxslt

!pip install pinecone-client

!pip install openai

!pip install tiktoken





!pip install unstructured_pytesseract

!pip install pytesseract

!pip install numpy==1.25.0

! apt install tesseract-ocr
! apt install libtesseract-dev

!wget -q https://github.com/Hey-Aryan/Documents-/archive/refs/heads/main.zip
!unzip main.zip

from langchain_community.document_loaders import UnstructuredFileLoader, PyPDFLoader

files = ("/content/Documents--main/Kentucky.pdf")

loader = UnstructuredFileLoader(files)

#pytesseract.pytesseract.tesseract_cmd = r'/usr/locallib/python3.10/dist-packages/pytesseract'

data = loader.load()

# Note: If you're using PyPDFLoader then it will split by page for you already
print (f'You have {len(data)} document(s) in your data')
print (f'There are {len(data[0].page_content)} characters in your sample document')
print (f'Here is a sample: {data[0].page_content[:200]}')

"""#Chunk your data up into smaller documents"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

# We'll split our data into chunks around 1000 characters each with a 100 character overlap. These are relatively small.

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
texts = text_splitter.split_documents(data)

print (f'Now you have {len(data)} documents')

text = data[1]
text

print(text.metadata["source"])

"""❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌
# Can't use Page No (metadata) in multiple types of Files
❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌❌
"""

text = texts[1]

metadata = {
    'char_length': len(text.page_content),
    'page_no': str(text.metadata["page"]),
    'text': text.page_content[:1000],
    'source': text.metadata["source"]
}

metadata

"""# Create embeddings of your documents"""

from langchain.vectorstores import Chroma, Pinecone
from langchain.embeddings.openai import OpenAIEmbeddings
import pinecone
import os

OPENAI_API_KEY = os.getenv('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', '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')

embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)

"""---


#upsert Data (ids, embeddings, metadata)


---
"""

import random
import itertools
from pinecone import Pinecone

def get_pinecone_index():
    pc = Pinecone(api_key="54fd135d-6e81-43c5-8802-ddfb63c09947")
    index = pc.Index("avivo-vector-db")
    return index

import time

index = get_pinecone_index()

def createVector(item):

    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-ada-002")
    text = texts[item].page_content[:1000]
    response = embeddings.embed_query(text)
    text = texts[item]


    metadata = {
    'char_length': len(text.page_content),
    'page_no': str(text.metadata["page"]),
    'text': text.page_content[:1000],
    'source': text.metadata["source"]
      }

    vector = []
    vector.append({"id": str(item), "values": response[:1536], "metadata": metadata})
    index.upsert(vector)


for item in range (len(texts)):
    print(item)
    createVector(item)

### texts[0] -- texts[112]

"""---
✅✅✅✅✅✅✅✅✅✅✅





# Retrieval



✅✅✅✅✅✅✅✅✅✅✅


---

"""

limit = 3750
import time
from openai import OpenAI

OPENAI_API_KEY='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'
client = OpenAI(api_key=OPENAI_API_KEY)

def retrieve(query):
    embed = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-ada-002")
    res = embeddings.embed_query(query)

    # retrieve from Pinecone
    xq = res[:1536]

    # get relevant contexts
    contexts = []
    time_waited = 0
    while (len(contexts) < 3 and time_waited < 60 * 12):
        res = index.query(vector=xq, top_k=3, include_metadata=True)
        contexts = contexts + [
            x['metadata']['text'] for x in res['matches']
        ]
        print(f"Retrieved {len(contexts)} contexts, sleeping for 15 seconds...")
        time.sleep(15)
        time_waited += 15

    if time_waited >= 60 * 12:
        print("Timed out waiting for contexts to be retrieved.")
        contexts = ["No contexts retrieved. Try to answer the question yourself!"]


    # build our prompt with the retrieved contexts included
    prompt_start = (
        "Answer the question based on the context below.\n\n"+
        "Context:\n"
    )
    prompt_end = (
        f"\n\nQuestion: {query}\nAnswer:"
    )
    # append contexts until hitting limit
    for i in range(1, len(contexts)):
        if len("\n\n---\n\n".join(contexts[:i])) >= limit:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts[:i-1]) +
                prompt_end
            )
            break
        elif i == len(contexts)-1:
            prompt = (
                prompt_start +
                "\n\n---\n\n".join(contexts) +
                prompt_end
            )
    return prompt

def complete(prompt):
    # instructions
    sys_prompt = "You are a helpful assistant that always answers questions."
    # query text-davinci-003
    res = client.chat.completions.create(
        model='gpt-3.5-turbo-0125',
        messages=[
            {"role": "system", "content": sys_prompt},
            {"role": "user", "content": prompt}
        ]
    )
    return res.choices[0].message.content

query = ("Which Type of gpu is it")
query_with_contexts = retrieve(query)
complete(query_with_contexts)

query = ("Tell me about PlayPredictor dataset")
query_with_contexts = retrieve(query)
complete(query_with_contexts)

query = ("Which is the first question in the image ?")
query_with_contexts = retrieve(query)
complete(query_with_contexts)

query = ("From the access of gpu is VALIDATED BY ")
query_with_contexts = retrieve(query)
complete(query_with_contexts)

query = ("List down the team members and guide name ")
query_with_contexts = retrieve(query)
complete(query_with_contexts)

query = ("Extract the below information from the document 1. License Number 2. Name of Medical practitoner 3. Profession (doctor, nurse, physiotherapist etc 4. Action effective date 5. Action Taken 6. Summary of action taken 7. Address (might be city/state/country 8. Date or year of birth. In clean Json format ")
query_with_contexts = retrieve(query)
complete(query_with_contexts)

query = ("What is the 1st question from Linux System Programming Assignment")
query_with_contexts = retrieve(query)
query_with_contexts